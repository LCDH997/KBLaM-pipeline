{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8ab91905-02ad-4749-9868-cbe64d8ddbd4",
   "metadata": {},
   "source": [
    "# LLM call"
   ]
  },
  {
   "cell_type": "code",
   "id": "2a0de559-a279-425e-831b-391ce1503f63",
   "metadata": {},
   "source": [
    "from typing import (Generic, List, Optional, Tuple, TypeVar, Any, AsyncIterator, \n",
    "                    Union, Sequence)\n",
    "from langchain_core.messages import BaseMessage\n",
    "from langchain_core.language_models.chat_models import BaseChatModel\n",
    "from langchain_core.output_parsers import PydanticOutputParser\n",
    "from langchain.output_parsers import RetryOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate, HumanMessagePromptTemplate, SystemMessagePromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.runnables import Runnable, RunnableLambda, RunnableParallel, RunnablePassthrough\n",
    "from langchain_core.prompt_values import PromptValue\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "import langchain.evaluation.qa.eval_chain as eval_chain\n",
    "import logging\n",
    "import pandas as pd\n",
    "from pydantic import BaseModel, Field"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "83b035fb-a218-4c7c-ac8a-e952d09826b2",
   "metadata": {},
   "source": [
    "MessageLikeRepresentation = Union[\n",
    "    BaseMessage, list[str], tuple[str, str], str, dict[str, Any]\n",
    "]\n",
    "LanguageModelInput = Union[PromptValue, str, Sequence[MessageLikeRepresentation]]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "3d362635-d4ae-48e0-b4f4-42659cee3822",
   "metadata": {},
   "source": [
    "class VLLMChatOpenAI(ChatOpenAI):\n",
    "    def _get_request_payload(\n",
    "        self,\n",
    "        input_: LanguageModelInput,\n",
    "        *,\n",
    "        stop: Optional[List[str]] = None,\n",
    "        **kwargs: Any,\n",
    "    ) -> dict:\n",
    "        payload = super()._get_request_payload(input_, stop=stop, **kwargs)\n",
    "        # max_tokens was deprecated in favor of max_completion_tokens\n",
    "        # in September 2024 release\n",
    "        if \"max_completion_tokens\" in payload:\n",
    "            payload[\"max_tokens\"] = payload.pop(\"max_completion_tokens\")\n",
    "        return payload"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "678998c2-bdf9-4f12-a8b7-d77ab9e63664",
   "metadata": {},
   "source": [
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def build_chain(llm: BaseChatModel, \n",
    "                system_template: str,\n",
    "                human_template: str,\n",
    "                parser: Optional[PydanticOutputParser] = None) -> Runnable:\n",
    "    prompt = ChatPromptTemplate(\n",
    "        messages=[\n",
    "            SystemMessagePromptTemplate.from_template([{\"text\": system_template}]),\n",
    "            HumanMessagePromptTemplate.from_template([{\"text\": human_template}])\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    chain = (\n",
    "        prompt\n",
    "        | RunnableParallel(completion=llm, prompt_value=RunnablePassthrough())\n",
    "    )\n",
    "\n",
    "    if parser:\n",
    "        retry_planner_parser = RetryOutputParser.from_llm(\n",
    "            parser=parser,\n",
    "            llm=llm,\n",
    "            prompt=PromptTemplate.from_template(\"{prompt}\"),\n",
    "            max_retries=3\n",
    "        )\n",
    "        \n",
    "        def _do_parsing_retrying(x: dict):\n",
    "                result = None\n",
    "                completion = x['completion'].content\n",
    "                prompt_value = x['prompt_value']\n",
    "\n",
    "                logger.info(f\"Trying structured parsing, Received completion: {completion}\")\n",
    "\n",
    "                try:\n",
    "                    result = retry_planner_parser.parse_with_prompt(completion=completion,prompt_value=prompt_value)\n",
    "                except OutputParserException as e:\n",
    "                    logger.warning(\"Proceeding without result due to parser errors (even after retrying). \"\n",
    "                                   \"Prompt - %s\" % prompt_value)\n",
    "                    raise e                    \n",
    "\n",
    "                return result\n",
    "\n",
    "        chain = (\n",
    "            RunnableLambda(lambda x: {**x, \"response_format_description\": parser.get_format_instructions()})\n",
    "            | chain\n",
    "            | RunnableLambda(_do_parsing_retrying, name=\"retry_planner_lambda\")\n",
    "        )\n",
    "\n",
    "    return chain"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "04fe1940-dcb2-4b07-a71a-a97076e11855",
   "metadata": {},
   "source": [
    "OPENAI_API_BASE = 'http://10.32.2.11:8041/v1'\n",
    "OPENAI_API_KEY = 'token-abc123'"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "0f57764f-1ce5-431a-8fa4-133723bf5cd7",
   "metadata": {},
   "source": [
    "llm = VLLMChatOpenAI(\n",
    "            model=\"/model\",\n",
    "            base_url=OPENAI_API_BASE,\n",
    "            api_key=OPENAI_API_KEY,\n",
    "            temperature=0.3,\n",
    "            max_tokens=8096,\n",
    "        )"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "66730bef-be3e-4359-9acf-6ebcc9024e6c",
   "metadata": {},
   "source": [
    "df = pd.read_json(\"ObliQA_trained.json\")\n",
    "#print(len(df))\n",
    "Original_Answers = [df[\"Or_A\"][i] for i in range(len(df[\"Or_A\"]))]\n",
    "Answers = [df[\"A\"][i] for i in range(len(df[\"A\"]))]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "ab1324a2-472b-45e1-86f4-de685bda0184",
   "metadata": {},
   "source": [
    "class ExtractedMeta(BaseModel):\n",
    "    correctness: Optional[float] = Field(default=None, description=\"Correctness score\")\n",
    "    coverage: Optional[float] = Field(default=None, description=\"Coverage score\")\n",
    "    clarity: Optional[float] = Field(default=None, description=\"Clarity score\")\n",
    "    overall_assessment: Optional[str] = Field(default=None, description=\"1–3 sentence summary of the comparison.\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "1a8c937f-b40a-4181-ad0e-8f632edb7a6f",
   "metadata": {},
   "source": [
    "system_template = \"\"\"Task:\n",
    "You are given two answers to the same question:\n",
    "Golden Answer – the correct or ideal reference answer.\n",
    "Generated Answer – the model-produced answer to evaluate.\n",
    "Your job is to compare the Generated Answer against the Golden Answer using the criteria below and then produce a short structured evaluation.\n",
    "\n",
    "Criteria (USE ONLY THESE CRITERIA):\n",
    "Correctness — Does the generated answer provide factually correct information compared to the golden answer?(NOTE: Being different from the golden answer is allowed if still correct.)\n",
    "Coverage of Key Details — Does the generated answer include the important points, constraints, and nuances mentioned in the golden answer?\n",
    "Clarity & Coherence — Is the answer well-structured, easy to understand, and logically flowing?\n",
    "\n",
    "Represent the answer as following:\n",
    "Correctness score - the float number from 0.0 to 1.0(the more - the better, where 1.0 - 100% correct answer)\n",
    "Coverage score - the float number from 0.0 to 1.0(the more - the better, where 1.0 - 100% details mentioned)\n",
    "Clarity score - the float number from 0.0 to 1.0(the more - the better, where 1.0 - 100% correct structure, understandable and logically correct)\n",
    "Overall assessment - summary of scoring.\n",
    "\n",
    "Follow the answer format:\n",
    "{response_format_description}\n",
    "\"\"\"\n",
    "\n",
    "human_template = \"\"\"REAL DATA: The following section is the real data. You should use only this real data to prepare your answer. Extract all the necessary information for the answer.\n",
    "Golden Answer: {answer}\n",
    "Model Answer: {model_answer}\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "parser = PydanticOutputParser(pydantic_object=ExtractedMeta)\n",
    "meta_chain = build_chain(llm, system_template, human_template, parser)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "6c8355d0-0402-4623-aa7d-59a9ce1f6c09",
   "metadata": {},
   "source": [
    "result = []\n",
    "for i in range(0, len(Original_Answers), 50):\n",
    "    result.append(await meta_chain.abatch([{\n",
    "            \"answer\": Original_Answers,\n",
    "            \"model_answer\": Answers,\n",
    "        } for j in range(min(50, len(Original_Answers)-i))] ))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "40bdee9f-68a7-4f30-b9c9-dc5652b53eb9",
   "metadata": {},
   "source": "print(result[0][0])",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "new_result = []\n",
    "for j in result:\n",
    "    for k in j:\n",
    "        new_result.append(k)\n",
    "print(new_result[0])"
   ],
   "id": "21a9a4a4526fe786",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "99ea26fc-3614-4124-99fc-ac3bf29fde48",
   "metadata": {},
   "source": [
    "Result_dict = []\n",
    "for i in range(len(new_result)):\n",
    "    Result_dict.append({\"Correctness\": new_result[i].correctness, \"Coverage\": new_result[i].coverage, \"Clarity\": new_result[i].clarity, \"Overall_assessment\": new_result[i].overall_assessment})\n",
    "print(Result_dict[0])\n",
    "import json\n",
    "with open('zeroshot_result.json', 'w') as fout:\n",
    "    json.dump(Result_dict, fout)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "fef0f6bc-7154-42b1-8b7a-05b2b4ca66ca",
   "metadata": {},
   "source": [
    "print(len(Result_dict))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "1035672e-26e7-4ad1-be88-91841b7737f6",
   "metadata": {},
   "source": [
    "df = pd.read_json(\"Metrics for unlearned.json\")\n",
    "print(df)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "avg_cor_u = 0\n",
    "avg_cov_u = 0\n",
    "avg_cla_u = 0\n",
    "avg_cor_l = 0\n",
    "avg_cov_l = 0\n",
    "avg_cla_l = 0\n",
    "for i in range(len(df[\"Correctness\"])):\n",
    "    avg_cor_u+=df[\"Correctness\"][i]/len(df[\"Correctness\"])\n",
    "    avg_cov_u+=df[\"Coverage\"][i]/len(df[\"Coverage\"])\n",
    "    avg_cla_u+=df[\"Clarity\"][i]/len(df[\"Clarity\"])\n",
    "for i in range(len(Result_dict)):\n",
    "    avg_cor_l+=Result_dict[i][\"Correctness\"]/len(Result_dict)\n",
    "    avg_cov_l+=Result_dict[i][\"Coverage\"]/len(Result_dict)\n",
    "    avg_cla_l+=Result_dict[i][\"Clarity\"]/len(Result_dict)\n",
    "print(f\"Average Correctness: {avg_cor_u} vs {avg_cor_l}\\nAverage Coverage: {avg_cov_u} vs {avg_cov_l}\\nAverage Clarity: {avg_cla_u} vs {avg_cla_l}\")"
   ],
   "id": "362724c18dfe9c8b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "811d83e0bafbebe",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
